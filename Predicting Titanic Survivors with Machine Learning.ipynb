{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "PassengerId    891\n",
      "Survived       891\n",
      "Pclass         891\n",
      "Name           891\n",
      "Sex            891\n",
      "Age            714\n",
      "SibSp          891\n",
      "Parch          891\n",
      "Ticket         891\n",
      "Fare           891\n",
      "Cabin          204\n",
      "Embarked       889\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y(survived), x(died,survived)\n",
    "fig1 = plt.figure(figsize(18,6))\n",
    "\n",
    "plt.subplot2grid((2,3),(0,0))\n",
    "df.Survived.value_counts(normalize=True).plot(kind=\"bar\", alpha = 0.5)\n",
    "plt.title(\"Survived\")\n",
    "# About ~60% died and ~40% survived\n",
    "\n",
    "# Since we have alot of the ages of the passengers, \n",
    "# we can take a look at the relationship between the age and the survivor rate.\n",
    "\n",
    "plt.subplot2grid((2,3),(0,1))\n",
    "plt.scatter(df.Survived, df.Age, alpha=0.1)\n",
    "plt.title(\"Age wrt Survived\")\n",
    "\n",
    "plt.subplot2grid((2,3),(0,2))\n",
    "df.Pclass.value_counts(normalize=True).plot(kind=\"bar\", alpha = 0.5)\n",
    "plt.title(\"Class\")\n",
    "\n",
    "plt.subplot2grid((2,3),(1,0), colspan=2)\n",
    "for x in [1,2,3]:\n",
    "    df.Age[df.Pclass == x].plot(kind = \"kde\") #kde = kernal density estimation\n",
    "plt.title(\"Class wrt Age\")\n",
    "plt.legend((\"1st\",\"2nd\",\"3rd\"))\n",
    "    # Seems like there is a relationship between the type of tickets they could buy\n",
    "    # you have and the average ages. To make sense of information, try to visualize it\n",
    "\n",
    "plt.subplot2grid((2,3),(1,2))\n",
    "df.Embarked.value_counts(normalize=True).plot(kind=\"bar\", alpha = 0.5)\n",
    "plt.title(\"Embarked\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize gender\n",
    "fig2 = plt.figure(figsize(18,12))\n",
    "\n",
    "# Amount of people that survived\n",
    "plt.subplot2grid((3,4),(0,0))\n",
    "df.Survived.value_counts(normalize=True).plot(kind=\"bar\", alpha = 0.5)\n",
    "plt.title(\"Survived\")\n",
    "\n",
    "# Male survival ratio (~20 % survived)\n",
    "plt.subplot2grid((3,4),(0,1))\n",
    "df.Survived[df.Sex == \"male\"].value_counts(normalize = True).plot(kind = \"bar\", alpha=0.5)\n",
    "plt.title(\"Men Survived\")\n",
    "\n",
    "# Female survival ratio (~70 % survived)\n",
    "female_color = \"#FA0000\"\n",
    "plt.subplot2grid((3,4),(0,2))\n",
    "df.Survived[df.Sex == \"female\"].value_counts(normalize = True).plot(kind = \"bar\", alpha=0.5, color = female_color)\n",
    "plt.title(\"Female Survived\")\n",
    "\n",
    "# Men vs. Womens survival ratio\n",
    "plt.subplot2grid((3,4),(0,3))\n",
    "df.Sex[df.Survived == 1].value_counts(normalize = True).plot(kind = \"bar\", alpha=0.5, color = [female_color, 'b'])\n",
    "plt.title(\"Sex of Survived\")\n",
    "\n",
    "# Going to use the Class of the passenger to show their survival rate\n",
    "plt.subplot2grid((3,4),(1,0), colspan = 4)\n",
    "for x in [1,2,3]:\n",
    "    df.Survived[df.Pclass == x].plot(kind = \"kde\")\n",
    "plt.title(\"Class wrt Survived\")\n",
    "plt.legend((\"1st\",\"2nd\",\"3rd\"))\n",
    "    # We see that the mortality rate among 3rd class passengers is much higher than that of the 2nd \n",
    "    # and 3rd class passengers. Most who did survive was people in 1st class, then 2nd and lastly 3rd.\n",
    "\n",
    "# What if you combine the information from Sex and the passenger Class?\n",
    "\n",
    "# Rich Men Survived\n",
    "plt.subplot2grid((3,4),(2,0)) \n",
    "df.Survived[(df.Sex == \"male\") & (df.Pclass == 1)].value_counts(normalize = True).plot(kind = \"bar\", alpha=0.5)\n",
    "plt.title(\" Rich Men Survived\")\n",
    "\n",
    "# Poor Men Survived (Jack)\n",
    "plt.subplot2grid((3,4),(2,1)) \n",
    "df.Survived[(df.Sex == \"male\") & (df.Pclass == 3)].value_counts(normalize = True).plot(kind = \"bar\", alpha=0.5)\n",
    "plt.title(\"Poor Men Survived\")\n",
    "\n",
    "# Rich Women Survived (Rose)\n",
    "plt.subplot2grid((3,4),(2,2)) \n",
    "df.Survived[(df.Sex == \"female\") & (df.Pclass == 1)].value_counts(normalize = True).plot(kind = \"bar\", alpha=0.5, color = female_color)\n",
    "plt.title(\" Rich Women Survived\")\n",
    "\n",
    "# Poor Women Survived\n",
    "plt.subplot2grid((3,4),(2,3)) \n",
    "df.Survived[(df.Sex == \"female\") & (df.Pclass == 3)].value_counts(normalize = True).plot(kind = \"bar\", alpha=0.5, color = female_color)\n",
    "plt.title(\"Poor Women Survived\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.786756\n",
      "0    0.213244\n",
      "Name: Result, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Use an algorithm to predict the data\n",
    "    # 1) The simplest algorithm would be random (50 % chanes of being correct)\n",
    "    #    => Simple rule based on graphs: If you are a women, you survive, if you are a man, you die.\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# add new column\n",
    "train[\"Hyp\"] = 0 # added a new column named \"hyp\", stands for hypothesis, all rows are filled with 0\n",
    "train.loc[train.Sex == \"female\", \"Hyp\"] = 1 # \"loc\" method: You pass the condition AND the column to update \n",
    "\n",
    "train[\"Result\"] = 0\n",
    "train.loc[train.Survived == train[\"Hyp\"], \"Result\"] = 1\n",
    "\n",
    "print(train[\"Result\"].value_counts(normalize = True))\n",
    "    # C: ALWAYS TAKE A LOOK ON THE DATA BEFORE RUNNING THE ALGORITHM: \n",
    "    #    'Sometimes, simple hints can give you really good answer'\n",
    "    #     Using more complicated algorithms will NOT gather a big increase in predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7912457912457912\n",
      "0.8237934904601572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahmu\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "    data[\"Fare\"] = data[\"Fare\"].fillna(data[\"Fare\"].dropna().median()) \n",
    "    # fill in empty rows which don't have the Fare data by average values for Fare\n",
    "    data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].dropna().median())\n",
    "    # fill in empty rows which don't have the Age data by average values for Age    \n",
    "    \n",
    "    # Machines don't work well with categories, give them numbers, assign \"male = 0\", \"female = 1\"\n",
    "    data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "    data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "    \n",
    "    data[\"Embarked\"] = data[\"Embarked\"].fillna(\"S\")\n",
    "    data.loc[data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "    data.loc[data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "    data.loc[data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2    \n",
    "\n",
    "# Linear models might seem trivial for humans in 2D whilst becoming virtually impossible in say, 15D\n",
    "# A good numeric approach beats the human brain. We are not good in dealing with, say 20D.\n",
    "\n",
    "# Image recognition tasks, usually assign different dimensions to each pixel, you will have an image\n",
    "# that is 50x50 pixels, i.e. 2500D, who can solve that? I can't.\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Predict logistic progession\n",
    "\n",
    "from sklearn import linear_model, preprocessing\n",
    "#from sklearn from linear_model\n",
    "# sklearn implements a bunch of machine learning algorithms, you don't have to do anything\n",
    "\n",
    "log_reg = linear_model.LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=7600)\n",
    "\n",
    "# The picture represents a linear model, it gathers a simple explanation of the datapoints\n",
    "#from PIL import Image\n",
    "#image = Image.open('linear_model.png')\n",
    "#image.show()\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "clean_data(train)\n",
    "\n",
    "# I have to pass in the desired output into the algorithm: Which usually is called 'target'\n",
    "target = train[\"Survived\"].values\n",
    "# Give the machine learning algorithm some hint, called features\n",
    "features = train[[\"Pclass\", \"Age\", \"Sex\", \"SibSp\", \"Parch\"]].values\n",
    "\n",
    "# 1 Feature (\"Sex\"): Gathered 78.67 % accuracy\n",
    "# 5 Features (\"Pclass\", \"Age\", \"Sex\", \"SibSp\", \"Parch\"): Gathered 79.12 % accuracy\n",
    "# 6 Features (\"Pclass\", \"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"): Gathered 79.57 % accuracy\n",
    "\n",
    "# Creating \"classifier\": Called classifier because it just needs to take one of these passengers \n",
    "# and decide which of these buckets(categories in features) he (the classifier) wants to assign\n",
    "# him into\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier_ = classifier.fit(features, target)\n",
    "# \"How many survivde based on these features, make a model\"\n",
    "\n",
    "# As soon as the PC sees the word \"fit\", it goes through the data \n",
    "# and tries to find some hidden relationships in the data\n",
    "\n",
    "# To check the accuracy of our model, i.e. \"fitted\" logical regression, we use \"score\"\n",
    "print(classifier_.score(features,target))\n",
    "\n",
    "# OUR METHODOLOGY: \"HERE YOU GO, A BUNCH OF CATEGORIES, GIVE ME A GOOD EXPLANATION OF THIS!\" \n",
    "\n",
    "# Take the linear features: transform and combine them into second degree polynomial\n",
    "# Maybe our data could be better described by curves instead of straight lines\n",
    "#from PIL import Image\n",
    "#image1 = Image1.open('linear_wrong.png')\n",
    "#image2 = Image2.open('quadratic.png')\n",
    "#image1.show()\n",
    "#image2.show()\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=2)\n",
    "poly_features = poly.fit_transform(features)\n",
    "\n",
    "classifier_ = classifier.fit(poly_features, target)\n",
    "print(classifier_.score(poly_features,target))\n",
    "\n",
    "# Alot of these algorithms are like \"black boxes\", you do not understand what happens in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9797979797979798\n",
      "[0.66666667 0.61111111 0.66666667 0.88888889 0.94444444 0.94444444\n",
      " 0.72222222 0.77777778 0.72222222 0.77777778 0.72222222 0.61111111\n",
      " 0.72222222 0.77777778 0.55555556 0.83333333 1.         0.66666667\n",
      " 0.77777778 0.77777778 0.88888889 0.77777778 0.88888889 0.72222222\n",
      " 0.55555556 0.83333333 0.94444444 0.88888889 0.66666667 0.83333333\n",
      " 0.72222222 0.66666667 0.88888889 0.94444444 0.88888889 0.77777778\n",
      " 0.72222222 0.72222222 0.72222222 0.77777778 0.88888889 0.82352941\n",
      " 0.70588235 0.82352941 0.82352941 0.70588235 0.82352941 0.82352941\n",
      " 0.88235294 0.88235294]\n",
      "0.7836601307189542\n"
     ]
    }
   ],
   "source": [
    "# PREDICT DECISION TREE\n",
    "# This algorithm creates a simple decision tree\n",
    "# It will take a look on all data, all rows, all the characteristics of it\n",
    "# And it will makes a decision tree, \"this move moves me the most in a certain direction\"\n",
    "\n",
    "from sklearn import tree, model_selection\n",
    "\n",
    "def clean_data(data):\n",
    "    data[\"Fare\"] = data[\"Fare\"].fillna(data[\"Fare\"].dropna().median()) \n",
    "    # fill in empty rows which don't have the Fare data by average values for Fare\n",
    "    data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].dropna().median())\n",
    "    # fill in empty rows which don't have the Age data by average values for Age    \n",
    "    \n",
    "    # Machines don't work well with categories, give them numbers, assign \"male = 0\", \"female = 1\"\n",
    "    data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "    data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "    \n",
    "    data[\"Embarked\"] = data[\"Embarked\"].fillna(\"S\")\n",
    "    data.loc[data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "    data.loc[data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "    data.loc[data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2   \n",
    "    \n",
    "log_reg = linear_model.LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=7600)\n",
    "    \n",
    "train = pd.read_csv(\"train.csv\")\n",
    "clean_data(train)\n",
    "\n",
    "target = train[\"Survived\"].values\n",
    "feature_names = [\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "features = train[feature_names].values\n",
    "\n",
    "decision_tree = tree.DecisionTreeClassifier(random_state = 1) \n",
    "decision_tree_ = decision_tree.fit(features, target)\n",
    "\n",
    "print(decision_tree_.score(features, target))\n",
    "\n",
    "#initilize with random number 1, derivatives are not below 1, easier for the tree to work.\n",
    "#from PIL import Image\n",
    "#image = Image.open('high_poly.png')\n",
    "#image.show()\n",
    "# The classifier just uses a tree, with a high degree polynomial, which has a high frequency,\n",
    "# so it gets many of the data points right. Since we have many data points, its always going to do OK\n",
    "\n",
    "# HOW DO WE FIX THIS PROBLEM WITH THE ALGORITHM?\n",
    "# - INTENTIONALLY WITHDRAW SOME INFORMATION: \"I have these 10 points, but I will only pass the \n",
    "#   algorithm 6 points. Because I know that the algorithm otherwise is going to get overzealous and\n",
    "#   read too much into the data. What I want the algorithm to do is to try to keep a generalization\"\n",
    "\n",
    "# ALGORITHM: model_selection (better than tree, does not have the same issues)\n",
    "\n",
    "# When you withdraw information, the part of the data that you are hiding, name: \"Cross-validation-set\"\n",
    "scores = model_selection.cross_val_score(decision_tree, features, target, scoring = 'accuracy', cv=50)\n",
    "print(scores)\n",
    "print(scores.mean()) #Low score, since we don't use all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8787878787878788\n",
      "[0.77777778 0.66666667 0.77777778 0.94444444 0.83333333 0.88888889\n",
      " 0.61111111 0.83333333 0.88888889 0.88888889 0.72222222 0.66666667\n",
      " 0.83333333 0.77777778 0.72222222 0.83333333 0.94444444 0.72222222\n",
      " 0.94444444 0.83333333 0.88888889 0.83333333 0.83333333 0.88888889\n",
      " 0.94444444 0.83333333 0.83333333 0.83333333 0.77777778 0.88888889\n",
      " 0.72222222 0.66666667 0.88888889 0.88888889 0.83333333 0.77777778\n",
      " 0.72222222 0.66666667 0.88888889 0.77777778 0.83333333 0.88235294\n",
      " 0.76470588 0.88235294 0.88235294 0.64705882 0.88235294 0.76470588\n",
      " 1.         0.94117647]\n",
      "0.8196078431372549\n"
     ]
    }
   ],
   "source": [
    "# generelized_tree\n",
    "# This algorithm creates a simple decision tree\n",
    "# It will take a look on all data, all rows, all the characteristics of it\n",
    "# And it will makes a decision tree, \"this move moves me the most in a certain direction\"\n",
    "\n",
    "from sklearn import tree, model_selection\n",
    "\n",
    "def clean_data(data):\n",
    "    data[\"Fare\"] = data[\"Fare\"].fillna(data[\"Fare\"].dropna().median()) \n",
    "    data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].dropna().median())\n",
    "    \n",
    "    data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "    data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "    \n",
    "    data[\"Embarked\"] = data[\"Embarked\"].fillna(\"S\")\n",
    "    data.loc[data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "    data.loc[data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "    data.loc[data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2   \n",
    "    \n",
    "log_reg = linear_model.LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=7600)\n",
    "    \n",
    "train = pd.read_csv(\"train.csv\")\n",
    "clean_data(train)\n",
    "\n",
    "target = train[\"Survived\"].values\n",
    "feature_names = [\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "features = train[feature_names].values\n",
    "\n",
    "generelized_tree = tree.DecisionTreeClassifier(\n",
    "    random_state = 1,\n",
    "    max_depth = 7, # want to create a tree which is deeper than 7 levels\n",
    "    min_samples_split = 2 # can controll the sample split: when he decides that he has to branch out\n",
    ") \n",
    "generelized_tree_ = generelized_tree.fit(features, target)\n",
    "\n",
    "print(generelized_tree_.score(features, target))\n",
    "\n",
    "scores = model_selection.cross_val_score(generelized_tree, features, target, scoring = 'accuracy', cv=50)\n",
    "print(scores)\n",
    "print(scores.mean()) #FIX: Tell algorithm to NOT read TOO MUCH into the data\n",
    "\n",
    "# OBSERVATIONS: The general version of the tree was less confident on the first run compared to the\n",
    "#               decision tree: 88 vs 98 %. Although: The general one gave a better performance overall\n",
    "\n",
    "# HARD PROBLEM IN MACHINE LEARNING: Hard to understand what the machine is thinking\n",
    "# LUCKILY FOR US: This information is visualizable within decision trees.\n",
    "\n",
    "tree.export_graphviz(generelized_tree_, feature_names=feature_names, out_file = \"tree.dot\")\n",
    "# \"type dot.png\" in C:\\Users\\Mahmu\\Documents\\General, you will see the content, NOT HUMAN FRIENDLY\n",
    "# HUMAN FRIENDLY TRANSFORMATION OF DOT FILE: dot -Tpng tree.dot > tree.png \n",
    "# To use \"dot\", you need to download the API and set its PATH variable\n",
    "# https://stackoverflow.com/questions/48243249/graphvizs-dot-tool-on-windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "for x in [1,3,2]:\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
